{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wavenet_1015_east.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfDn1Z4huLJZ",
        "colab_type": "code",
        "outputId": "ccae9fa0-4126-4522-e7c7-5f4075342ce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        }
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import sklearn\n",
        "import sys\n",
        "import io\n",
        "\n",
        "def plot_learning_curves(loss, val_loss):\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(len(loss)), loss, \"b.-\", label=\"Training loss\")\n",
        "    plt.plot(np.arange(len(val_loss)), val_loss, \"r.-\", label=\"Validation loss\")\n",
        "    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
        "    plt.legend(fontsize=14)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)\n",
        "\n",
        "n_steps = 50\n",
        "forecast = 10\n",
        "\n",
        "#EXTRACT FLOW, Z SCORE, OUTLIERS\n",
        "data_west = pd.read_csv('denoised_data_1015_east.csv')\n",
        "data_west_o = np.array(data_west.FLOW)\n",
        "\n",
        "#EXRTEND DATA\n",
        "array_to_concatinate = data_west_o\n",
        "for iter in range (35):\n",
        "    data_west_o = np.concatenate([data_west_o,array_to_concatinate])\n",
        "\n",
        "#SCALE AND RESHAPE DATA\n",
        "scaler = MinMaxScaler()\n",
        "array = data_west_o.reshape(-1, 1)\n",
        "array_scaled = scaler.fit_transform(array)\n",
        "\n",
        "flow_reshaped = array_scaled[:(len(array_scaled) - (len(array_scaled) % (n_steps+forecast)))].reshape(-1, (n_steps+forecast), 1)\n",
        "#TRAIN SET, VALIDATION SET, TEST SET\n",
        "test = int(0.7 * flow_reshaped.shape[0])\n",
        "valid = int(0.9 * flow_reshaped.shape[0])\n",
        "\n",
        "X_train = flow_reshaped[:test, :n_steps]\n",
        "X_valid = flow_reshaped[test:valid, :n_steps]\n",
        "X_test = flow_reshaped[valid:, :n_steps]\n",
        "print(X_test.shape)\n",
        "print(X_test[-1:].shape)\n",
        "\n",
        "#prepare targets\n",
        "Y = np.empty((flow_reshaped.shape[0], n_steps, forecast))\n",
        "for step_ahead in range(1, forecast + 1):\n",
        "    Y[:, :, step_ahead - 1] = flow_reshaped[:, step_ahead:step_ahead + n_steps, 0]\n",
        "\n",
        "y_train = Y[:test]\n",
        "y_valid = Y[test:valid]\n",
        "y_test = Y[valid:]\n",
        "\n",
        "#MODEL\n",
        "def last_time_step_mse(Y_true, Y_pred):\n",
        "     return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
        "for rate in (1, 2, 4, 8) * 2:\n",
        "    model.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n",
        "                                  activation=\"relu\", dilation_rate=rate))\n",
        "model.add(keras.layers.Conv1D(filters=10, kernel_size=1))\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
        "\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, epochs=1000,\n",
        "                    validation_data=(X_valid, y_valid), callbacks=[early_stopping_cb])\n",
        "\n",
        "model.save(\"vawenet_1015_east.h5\")\n",
        "plot_learning_curves(history.history[\"loss\"], history.history[\"val_loss\"])\n",
        "\n",
        "#50 MIN FORECAST\n",
        "#50 MIN FORECAST\n",
        "#flow\n",
        "flow_unscaled = array[:(len(array) - (len(array) % (n_steps + forecast)))].reshape(-1, (n_steps + forecast), 1)\n",
        "y_test_unscaled = flow_unscaled[valid:, n_steps:, 0]\n",
        "y_real_rescaled = y_test_unscaled[-1, :].reshape(-1, 1)\n",
        "print(y_real_rescaled.shape)\n",
        "\n",
        "flow_not_reshaped = array[:(len(array) - (len(array) % (n_steps+forecast)))]\n",
        "\n",
        "#flow prediction\n",
        "y_pred = model.predict(X_test[-1, :].reshape(-1, n_steps, 1)) #shape (1, 50, 10)\n",
        "y_pred = y_pred[-1,-1,:].reshape(-1,1)\n",
        "y_pred_rescaled = scaler.inverse_transform(y_pred).reshape(-1, 1) #shape (10, 1)\n",
        "print(y_pred_rescaled.shape)\n",
        "\n",
        "#time\n",
        "time_not_reshaped = np.array(data_west['TIME'][:(len(data_west['TIME']) - (len(data_west['TIME']) % (n_steps+forecast)))])\n",
        "time_reshaped = np.array(data_west['TIME'][:(len(data_west['TIME']) - (len(data_west['TIME']) % (n_steps+forecast)))]).\\\n",
        "    reshape(-1, (n_steps+forecast), 1)\n",
        "\n",
        "valid_time = int(0.9 * time_reshaped.shape[0])\n",
        "y_time_test = time_reshaped[valid_time:, n_steps:, 0]\n",
        "print(y_time_test[-1, :].shape)\n",
        "\n",
        "def plot_prediction(y_real_resacled, y_pred_rescaled, flow_not_reshaped, time_not_reshaped, y_time_test):\n",
        "    plt.figure()\n",
        "    plt.title(\"50 minutes prediction\", fontsize=14)\n",
        "    plt.plot(time_not_reshaped[-300:-forecast], flow_not_reshaped[-300:-forecast], 'b-')\n",
        "    plt.plot(y_time_test[-1, :], y_real_resacled, 'ro-', label = 'Real values')\n",
        "    plt.plot(y_time_test[-1, :], y_pred_rescaled, 'gx-', label = 'Predicted values')\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.xlabel(\"Time (in 5 minutes intervals)\")\n",
        "    plt.ylabel('Volume (veh/h)')\n",
        "\n",
        "plot_prediction(y_real_rescaled, y_pred_rescaled, flow_not_reshaped, time_not_reshaped, y_time_test)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1072, 50, 1)\n",
            "(1, 50, 1)\n",
            "Train on 7499 samples, validate on 2142 samples\n",
            "Epoch 1/1000\n",
            "7499/7499 [==============================] - 5s 715us/sample - loss: 0.0432 - last_time_step_mse: 0.0404 - val_loss: 0.0094 - val_last_time_step_mse: 0.0096\n",
            "Epoch 2/1000\n",
            "7499/7499 [==============================] - 5s 617us/sample - loss: 0.0088 - last_time_step_mse: 0.0087 - val_loss: 0.0083 - val_last_time_step_mse: 0.0081\n",
            "Epoch 3/1000\n",
            "7499/7499 [==============================] - 5s 620us/sample - loss: 0.0080 - last_time_step_mse: 0.0076 - val_loss: 0.0075 - val_last_time_step_mse: 0.0068\n",
            "Epoch 4/1000\n",
            "7499/7499 [==============================] - 5s 630us/sample - loss: 0.0074 - last_time_step_mse: 0.0067 - val_loss: 0.0073 - val_last_time_step_mse: 0.0067\n",
            "Epoch 5/1000\n",
            " 704/7499 [=>............................] - ETA: 3s - loss: 0.0074 - last_time_step_mse: 0.0062"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3fcaab120557>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mearly_stopping_cb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m history = model.fit(X_train, y_train, epochs=1000,\n\u001b[0;32m---> 75\u001b[0;31m                     validation_data=(X_valid, y_valid), callbacks=[early_stopping_cb])\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vawenet_1015_east.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}